{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0d9e0",
   "metadata": {},
   "source": [
    "#### Check gpus availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99b5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# check gpus availability\n",
    "import torch\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")  \n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95db5",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e542ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,)\n",
    "\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57eb61",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb13288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and dataset configuration\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_path = \"fka/awesome-chatgpt-prompts\"\n",
    "working_dir = './'\n",
    "output_dir = os.path.join(working_dir, \"unsloth_lab_outputs\")\n",
    "\n",
    "# Training parameters - Proven stable hyperparameters from memory\n",
    "learning_rate = 2e-5  # Conservative learning rate for stability\n",
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "max_seq_length = 2048\n",
    "warmup_steps = 50\n",
    "weight_decay = 0.1  # Strong regularization\n",
    "max_grad_norm = 0.3  # Conservative gradient clipping\n",
    "\n",
    "# LoRA parameters - Reduced for stability\n",
    "lora_r = 16  # Reduced rank for stability\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.2\n",
    "'''\n",
    "target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    "'''\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Early stopping and stability\n",
    "early_stopping_patience = 4\n",
    "eval_steps = 50\n",
    "save_steps = 100\n",
    "\n",
    "# Weights & Biases\n",
    "#wandb_project = \"llama-3.1-8b-instruct-sg-legislation\"\n",
    "wandb_project = \"llama-3-8B-finetuning\"\n",
    "wandb_run_name = None\n",
    "\n",
    "# Other parameters\n",
    "resume_from_checkpoint = None\n",
    "test_dataset_split = 0.2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6e60a",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ad8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"unsloth_finetuning.log\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8c95c",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32ab28",
   "metadata": {},
   "source": [
    "#### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582d9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(dataset_path, tokenizer):\n",
    "    \"\"\"Load and prepare dataset.\"\"\"\n",
    "    logger.info(f\"Loading dataset from {dataset_path}\")\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_path)\n",
    "        logger.info(f\"Dataset loaded successfully: {dataset}\")\n",
    "\n",
    "        dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "        dataset = dataset[\"train\"]\n",
    "\n",
    "        dataset = dataset.remove_columns('act')\n",
    "\n",
    "        # Verify dataset structure\n",
    "        logger.info(\n",
    "            (\n",
    "            f\"Sample formatted dataset: \"\n",
    "            f\"{dataset[:1]}...\" )\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc997a",
   "metadata": {},
   "source": [
    "#### Load foundation model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e45e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_unsloth(model_name, max_seq_length=2048, load_in_4bit=False, dtype=None):\n",
    "    \"\"\" Load model and tokenizer with Unsloth optimizations \"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map=\"auto\"    \n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724ceb8",
   "metadata": {},
   "source": [
    "#### PEFT version of the foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cc0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peft_model(model, target_modules, lora_r=16, lora_alpha=16, lora_dropout=0.0, bias=\"none\", random_state=42):\n",
    "    \"\"\" Set LoRA config and return PEFT model \"\"\"\n",
    "\n",
    "    peft_model = FastLanguageModel.get_peft_model(\n",
    "        model=model,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        bias=bias,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cbbbe",
   "metadata": {},
   "source": [
    "#### Create training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c516ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_arguments(output_dir, learning_rate, num_epochs, batch_size=4,\n",
    "                              warmup_steps=0, weight_decay=0., \n",
    "                              lr_scheduler_type='linear', max_grad_norm=1.0,\n",
    "                              gradient_accumulation_steps=1, run_name=None, \n",
    "                              logging_steps=500, eval_steps=250, save_steps=500, seed=42):\n",
    "    \"\"\" Create training args\"\"\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        warmup_steps=warmup_steps,\n",
    "        lr_scheduler_type=lr_scheduler_type,  # Stable cosine scheduler\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=\"steps\",  # Fixed: was evaluation_strategy\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=run_name,\n",
    "        seed=seed,\n",
    "        data_seed=seed,\n",
    "        # RTX optimizations\n",
    "        #bf16=True,  # Use bf16 to match model precision\n",
    "        #fp16=False,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "        # Stability improvements\n",
    "        save_safetensors=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de84f3",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1091b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, training_args, \n",
    "                   train_dataset, eval_dataset = None,\n",
    "                   max_seq_length=2048, dataset_text_field=\"text\",\n",
    "                   callbacks=[]):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=dataset_text_field,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,\n",
    "            \"append_concat_token\": False,\n",
    "        },\n",
    "        callbacks=callbacks,        \n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ee75b",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6433b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StabilityCallback(TrainerCallback):\n",
    "    \"\"\"Callback for training stability and divergence detection.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        divergence_threshold=1.3,\n",
    "        min_steps_before_check=100,\n",
    "        patience=2,\n",
    "        no_improvement_patience=5,\n",
    "        gradient_explosion_threshold=5.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.divergence_threshold = divergence_threshold\n",
    "        self.min_steps_before_check = min_steps_before_check\n",
    "        self.patience = patience\n",
    "        self.no_improvement_patience = no_improvement_patience\n",
    "        self.gradient_explosion_threshold = gradient_explosion_threshold\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.divergence_count = 0\n",
    "        self.no_improvement_count = 0\n",
    "        self.validation_losses = []\n",
    "        self.gradient_norms = []\n",
    "        self.consecutive_high_gradients = 0\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Monitor gradient norms and training stability.\"\"\"\n",
    "        if logs:\n",
    "            # Track gradient norms\n",
    "            if \"grad_norm\" in logs:\n",
    "                self.gradient_norms.append(logs[\"grad_norm\"])\n",
    "\n",
    "                # Check for gradient explosion\n",
    "                if logs[\"grad_norm\"] > self.gradient_explosion_threshold:\n",
    "                    self.consecutive_high_gradients += 1\n",
    "                    logger.error(\n",
    "                        f\"\"\"Gradient explosion detected: {\n",
    "                            logs['grad_norm']:.4f} > {\n",
    "                            self.gradient_explosion_threshold}\"\"\"\n",
    "                    )\n",
    "\n",
    "                    if self.consecutive_high_gradients >= 3:\n",
    "                        logger.error(\n",
    "                            (\n",
    "                            \"Stopping training due to persistent \"\n",
    "                            \"gradient explosion\"\n",
    "                        )\n",
    "                        )\n",
    "                        control.should_training_stop = True\n",
    "                        return\n",
    "                elif logs[\"grad_norm\"] > 1.0:\n",
    "                    logger.warning(\n",
    "                        f\"\"\"High gradient norm detected: {\n",
    "                            logs['grad_norm']:.4f}\"\"\"\n",
    "                    )\n",
    "                    self.consecutive_high_gradients = max(\n",
    "                        0, self.consecutive_high_gradients - 1\n",
    "                    )\n",
    "                else:\n",
    "                    self.consecutive_high_gradients = 0\n",
    "\n",
    "                # Log gradient statistics\n",
    "                if len(self.gradient_norms) >= 10:\n",
    "                    recent_norms = self.gradient_norms[-10:]\n",
    "                    avg_norm = np.mean(recent_norms)\n",
    "                    max_norm = np.max(recent_norms)\n",
    "\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"gradient_norm_avg_10\": avg_norm,\n",
    "                            \"gradient_norm_max_10\": max_norm,\n",
    "                            \"gradient_norm_current\": logs[\"grad_norm\"],\n",
    "                            \"consecutive_high_gradients\":\n",
    "                                self.consecutive_high_gradients,\n",
    "                        },\n",
    "                        step=state.global_step,\n",
    "                    )\n",
    "\n",
    "            # Check for NaN/Inf in training loss\n",
    "            if \"train_loss\" in logs:\n",
    "                if np.isnan(logs[\"train_loss\"]) or np.isinf(\n",
    "                    logs[\"train_loss\"]\n",
    "                ):\n",
    "                    logger.error(\n",
    "                        (\n",
    "                        f\"NaN/Inf training loss detected at \"\n",
    "                        f\"step {state.global_step}\"\n",
    "                    )\n",
    "                    )\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Callback for divergence detection and early stopping.\"\"\"\n",
    "        if (\n",
    "            logs\n",
    "            and \"eval_loss\" in logs\n",
    "            and state.global_step >= self.min_steps_before_check\n",
    "        ):\n",
    "            current_val_loss = logs[\"eval_loss\"]\n",
    "            self.validation_losses.append(current_val_loss)\n",
    "\n",
    "            # Check for improvement\n",
    "            if current_val_loss < self.best_val_loss:\n",
    "                improvement = self.best_val_loss - current_val_loss\n",
    "                self.best_val_loss = current_val_loss\n",
    "                self.divergence_count = 0\n",
    "                self.no_improvement_count = 0\n",
    "                logger.info(\n",
    "                    f\"\"\"New best validation loss: {\n",
    "                        self.best_val_loss:.4f} (improvement: {\n",
    "                        improvement:.4f})\"\"\"\n",
    "                )\n",
    "            else:\n",
    "                self.no_improvement_count += 1\n",
    "\n",
    "                # Check for divergence\n",
    "                if (\n",
    "                    current_val_loss\n",
    "                    > self.best_val_loss * self.divergence_threshold\n",
    "                ):\n",
    "                    self.divergence_count += 1\n",
    "                    logger.warning(\n",
    "                        f\"\"\"Potential divergence detected: {\n",
    "                            current_val_loss:.4f} > {\n",
    "                            self.best_val_loss *\n",
    "                            self.divergence_threshold:.4f} (count: {\n",
    "                            self.divergence_count})\"\"\"\n",
    "                    )\n",
    "\n",
    "                    if self.divergence_count >= self.patience:\n",
    "                        logger.error(\n",
    "                            f\"\"\"Training diverged! Stopping at step {\n",
    "                                state.global_step}\"\"\"\n",
    "                        )\n",
    "                        control.should_training_stop = True\n",
    "                        return\n",
    "\n",
    "                # Check for no improvement\n",
    "                if (\n",
    "                    self.no_improvement_count\n",
    "                    >= self.no_improvement_patience\n",
    "                ):\n",
    "                    logger.warning(\n",
    "                        (\n",
    "                        (f\"No improvement for {self.no_improvement_count} \"\n",
    "                         f\"evaluations. Stopping training.\")\n",
    "                    )\n",
    "                    )\n",
    "                    control.should_training_stop = True\n",
    "                    return\n",
    "\n",
    "            # Calculate perplexity\n",
    "            perplexity = math.exp(current_val_loss)\n",
    "\n",
    "            # Enhanced logging\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"eval_perplexity\": perplexity,\n",
    "                    \"best_val_loss\": self.best_val_loss,\n",
    "                    \"divergence_count\": self.divergence_count,\n",
    "                    \"no_improvement_count\": self.no_improvement_count,\n",
    "                    \"val_loss_trend\": (\n",
    "                        current_val_loss - self.validation_losses[-2]\n",
    "                        if len(self.validation_losses) >= 2\n",
    "                        else 0\n",
    "                    ),\n",
    "                },\n",
    "                step=state.global_step,\n",
    "            )\n",
    "\n",
    "            logger.info(\n",
    "                f\"\"\"Step {\n",
    "                    state.global_step}: Validation Loss: {\n",
    "                    current_val_loss:.4f}, Perplexity: {\n",
    "                    perplexity:.2f}, No improvement: {\n",
    "                    self.no_improvement_count}\"\"\"\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967babb",
   "metadata": {},
   "source": [
    "#### Get latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adbe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    checkpoints = []\n",
    "    for item in os.listdir(output_dir):\n",
    "        if item.startswith(\"checkpoint-\") and os.path.isdir(\n",
    "            os.path.join(output_dir, item)\n",
    "        ):\n",
    "            try:\n",
    "                step_num = int(item.split(\"-\")[1])\n",
    "                checkpoints.append(\n",
    "                    (step_num, os.path.join(output_dir, item))\n",
    "                )\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "    if checkpoints:\n",
    "        # Return the checkpoint with the highest step number\n",
    "        latest_step, latest_path = max(checkpoints, key=lambda x: x[0])\n",
    "        logger.info(\n",
    "            f\"Found latest checkpoint: {latest_path} (step {latest_step})\"\n",
    "        )\n",
    "        return latest_path\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a748f06",
   "metadata": {},
   "source": [
    "#### System infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1702ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system_info():\n",
    "    \"\"\"System information.\"\"\"\n",
    "    logger.info(\"=== System Information ===\")\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"GPU memory: { torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "    logger.info(\"=== End System Information ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02117c",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef181299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Print system information\n",
    "    print_system_info()\n",
    "\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initialize Weights & Biases\n",
    "    run_name = (\n",
    "        wandb_run_name\n",
    "        or f\"llama-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model_name\": model_name,\n",
    "            \"dataset_path\": dataset_path,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"batch_size\": \"auto\",\n",
    "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            \"max_seq_length\": max_seq_length,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"max_grad_norm\": max_grad_norm,\n",
    "            \"lora_r\": lora_r,\n",
    "            \"lora_alpha\": lora_alpha,\n",
    "            \"lora_dropout\": lora_dropout,\n",
    "            \"early_stopping_patience\": early_stopping_patience,\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    logger.info(\n",
    "        f\"Starting {model_name} fine-tuning ...\"\n",
    "    )\n",
    "    logger.info(\n",
    "        (f\"Fixed parameters: model={model_name}, lr={learning_rate}, \"\n",
    "         f\"epochs={num_epochs}\")\n",
    "    )\n",
    "    \n",
    "    # Auto-detect checkpoint if not specified\n",
    "    resume_checkpoint = resume_from_checkpoint\n",
    "    if resume_checkpoint is None:\n",
    "        latest_checkpoint = find_latest_checkpoint(output_dir)\n",
    "        if latest_checkpoint:\n",
    "            logger.info(\n",
    "                (\n",
    "                f\"Auto-resuming from latest checkpoint: \"\n",
    "                f\"{latest_checkpoint}\"\n",
    "            )\n",
    "            )\n",
    "            resume_checkpoint = latest_checkpoint\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"No existing checkpoints found. Starting fresh training.\"\n",
    "            )\n",
    "    elif resume_checkpoint and not os.path.exists(\n",
    "        resume_checkpoint\n",
    "    ):\n",
    "        logger.warning(\n",
    "            (f\"Checkpoint {resume_checkpoint} not found. \"\n",
    " \"Starting fresh training.\")\n",
    "        )\n",
    "        resume_checkpoint = None\n",
    "\n",
    "    \n",
    "    # Load model and tokenizer with Unsloth optimizations\n",
    "    logger.info(\"Loading model and tokenizer with Unsloth...\")\n",
    "    model, tokenizer = load_model_unsloth(model_name=model_name, \n",
    "                                          max_seq_length=max_seq_length, \n",
    "                                          load_in_4bit=True, \n",
    "                                          dtype=None)\n",
    "        \n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_and_prepare_dataset(dataset_path, tokenizer)\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    dataset = dataset.train_test_split(test_size=test_dataset_split)\n",
    "\n",
    "    # Log dataset information\n",
    "    logger.info(f\"Training samples: {len(dataset['train'])}\")\n",
    "    logger.info(f\"Validation samples: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Create PEFT vesion of the foundation model\n",
    "    model = load_peft_model(model=model, \n",
    "                            target_modules=target_modules, \n",
    "                            lora_r=lora_r, \n",
    "                            lora_alpha=lora_alpha, \n",
    "                            lora_dropout=lora_dropout, \n",
    "                            bias=\"none\", \n",
    "                            random_state=seed)    \n",
    "\n",
    "    # Set Training Args\n",
    "    training_args = create_training_arguments(output_dir=output_dir, \n",
    "                                              learning_rate=learning_rate, \n",
    "                                              num_epochs=num_epochs, \n",
    "                                              batch_size=batch_size,\n",
    "                                              warmup_steps=warmup_steps, \n",
    "                                              weight_decay=weight_decay,\n",
    "                                              lr_scheduler_type='cosine', \n",
    "                                              max_grad_norm=max_grad_norm,\n",
    "                                              gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "                                              run_name=run_name,\n",
    "                                              logging_steps=50, \n",
    "                                              eval_steps=eval_steps, \n",
    "                                              save_steps=save_steps, \n",
    "                                              seed=seed)    \n",
    "    \n",
    "    # Initialize callbacks with enhanced stability monitoring\n",
    "    stability_callback = StabilityCallback(\n",
    "        divergence_threshold=1.3,  # More conservative threshold\n",
    "        min_steps_before_check=100,  # Check earlier\n",
    "        patience=2,  # Less patience for divergence\n",
    "        no_improvement_patience=5,  # Stop if no improvement\n",
    "        gradient_explosion_threshold=5.0,  # Stop if gradients exceed 5.0\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "    )    \n",
    "\n",
    "    # Set Trainer\n",
    "    logger.info(\"Initializing trainer...\")    \n",
    "    trainer = create_trainer(model=model, \n",
    "                             tokenizer=tokenizer, \n",
    "                             training_args=training_args,\n",
    "                             train_dataset=dataset['train'], \n",
    "                             eval_dataset = dataset['test'],\n",
    "                             max_seq_length=max_seq_length, \n",
    "                             dataset_text_field=\"prompt\",\n",
    "                             callbacks=[stability_callback, early_stopping_callback]\n",
    "                             )    \n",
    "    \n",
    "    # Train\n",
    "    logger.info(\"Starting training...\")\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "        logger.info(\"Training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        raise    \n",
    "    \n",
    "    # Save the model\n",
    "    logger.info(\"Saving model...\")\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free some memory\n",
    "    del model\n",
    "    del trainer\n",
    "    del dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Finalize\n",
    "    logger.info(f\"Model saved to {output_dir}\")\n",
    "    logger.info(\"Fine-tuning completed successfully!\")\n",
    "\n",
    "    # Finish wandb\n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a86d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 08:45:23,411 - INFO - === System Information ===\n",
      "2025-09-02 08:45:23,412 - INFO - Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]\n",
      "2025-09-02 08:45:23,412 - INFO - PyTorch version: 2.8.0+cu128\n",
      "2025-09-02 08:45:23,413 - INFO - CUDA available: True\n",
      "2025-09-02 08:45:23,413 - INFO - CUDA version: 12.8\n",
      "2025-09-02 08:45:23,414 - INFO - GPU count: 1\n",
      "2025-09-02 08:45:23,415 - INFO - GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "2025-09-02 08:45:23,416 - INFO - GPU memory: 16.0 GB\n",
      "2025-09-02 08:45:23,416 - INFO - === End System Information ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilverkonlambigue\u001b[0m (\u001b[33msilverkonlambigue-skd\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/llm-finetuning/wandb/run-20250902_084523-2s0700tj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning/runs/2s0700tj' target=\"_blank\">llama-20250902-084523</a></strong> to <a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning/runs/2s0700tj' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning/runs/2s0700tj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 08:45:25,268 - INFO - Starting meta-llama/Meta-Llama-3-8B fine-tuning ...\n",
      "2025-09-02 08:45:25,269 - INFO - Fixed parameters: model=meta-llama/Meta-Llama-3-8B, lr=2e-05, epochs=100\n",
      "2025-09-02 08:45:25,272 - INFO - No existing checkpoints found. Starting fresh training.\n",
      "2025-09-02 08:45:25,273 - INFO - Loading model and tokenizer with Unsloth...\n",
      "==((====))==  Unsloth 2025.7.2: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "2025-09-02 08:45:27,626 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-09-02 08:45:37,946 - INFO - Loading dataset from fka/awesome-chatgpt-prompts\n",
      "2025-09-02 08:45:39,567 - INFO - Dataset loaded successfully: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['act', 'prompt'],\n",
      "        num_rows: 203\n",
      "    })\n",
      "})\n",
      "2025-09-02 08:45:39,657 - INFO - Sample formatted dataset: {'prompt': ['Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'], 'input_ids': [[128000, 52157, 499, 527, 459, 10534, 35046, 16131, 51920, 449, 6968, 264, 7941, 5226, 369, 264, 18428, 50596, 13, 578, 16945, 374, 311, 3665, 6743, 389, 279, 18428, 11, 3339, 1124, 34898, 320, 898, 8, 311, 5127, 11, 47005, 320, 2039, 8, 1193, 311, 279, 1732, 889, 27167, 279, 5226, 11, 323, 311, 1797, 1268, 1690, 3115, 279, 1984, 574, 6177, 13, 8000, 264, 22925, 488, 7941, 5226, 369, 420, 7580, 11, 2737, 279, 5995, 5865, 323, 38864, 369, 32145, 279, 5300, 9021, 13, 5321, 3493, 279, 2082, 323, 904, 9959, 41941, 311, 6106, 264, 2867, 8830, 315, 279, 8292, 13]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}...\n",
      "2025-09-02 08:45:39,664 - INFO - Training samples: 162\n",
      "2025-09-02 08:45:39,665 - INFO - Validation samples: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.7.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 08:45:42,079 - INFO - Initializing trainer...\n",
      "2025-09-02 08:45:42,509 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 162 | Num Epochs = 100 | Total steps = 2,100\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 6,815,744 of 8,037,076,992 (0.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 600/2100 08:54 < 22:20, 1.12 it/s, Epoch 28/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.733300</td>\n",
       "      <td>2.699003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.563200</td>\n",
       "      <td>2.409425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.302900</td>\n",
       "      <td>2.216940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.139500</td>\n",
       "      <td>2.129347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.040600</td>\n",
       "      <td>2.108090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.985400</td>\n",
       "      <td>2.094071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.918600</td>\n",
       "      <td>2.091607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.871000</td>\n",
       "      <td>2.091548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.797200</td>\n",
       "      <td>2.106251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.769200</td>\n",
       "      <td>2.114968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.712700</td>\n",
       "      <td>2.143961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.649600</td>\n",
       "      <td>2.161483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 08:47:59,643 - WARNING - High gradient norm detected: 1.0928\n",
      "2025-09-02 08:48:42,956 - WARNING - High gradient norm detected: 1.3729\n",
      "2025-09-02 08:49:28,566 - WARNING - High gradient norm detected: 1.2293\n",
      "2025-09-02 08:50:11,053 - WARNING - High gradient norm detected: 1.8334\n",
      "2025-09-02 08:50:57,681 - WARNING - High gradient norm detected: 1.7553\n",
      "2025-09-02 08:51:40,510 - WARNING - High gradient norm detected: 1.9941\n",
      "2025-09-02 08:52:26,607 - WARNING - High gradient norm detected: 2.4659\n",
      "2025-09-02 08:53:10,478 - WARNING - High gradient norm detected: 2.9562\n",
      "2025-09-02 08:53:54,977 - WARNING - High gradient norm detected: 3.7258\n",
      "2025-09-02 08:54:39,466 - WARNING - High gradient norm detected: 3.1714\n",
      "2025-09-02 08:54:42,185 - INFO - Training completed successfully!\n",
      "2025-09-02 08:54:42,186 - INFO - Saving model...\n",
      "2025-09-02 08:54:42,975 - INFO - Model saved to ./unsloth_lab_outputs\n",
      "2025-09-02 08:54:42,976 - INFO - Fine-tuning completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>consecutive_high_gradients</td><td>▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▂▁▁▁▁▁▁▁▂▂</td></tr><tr><td>eval/runtime</td><td>█▁▂▂▂▂▁▂▁▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇▇▇▇█▇█▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁█▇▇▇▇█▇█▇▇▇</td></tr><tr><td>gradient_norm_avg_10</td><td>▁▅█</td></tr><tr><td>gradient_norm_current</td><td>▁█▃</td></tr><tr><td>gradient_norm_max_10</td><td>▁██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▃▃▄▄▄▅▆█▇</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>consecutive_high_gradients</td><td>0</td></tr><tr><td>eval/loss</td><td>2.16148</td></tr><tr><td>eval/runtime</td><td>1.5834</td></tr><tr><td>eval/samples_per_second</td><td>25.893</td></tr><tr><td>eval/steps_per_second</td><td>6.947</td></tr><tr><td>gradient_norm_avg_10</td><td>2.15972</td></tr><tr><td>gradient_norm_current</td><td>3.17144</td></tr><tr><td>gradient_norm_max_10</td><td>3.72582</td></tr><tr><td>total_flos</td><td>3.0603010736553984e+16</td></tr><tr><td>train/epoch</td><td>28.58537</td></tr><tr><td>train/global_step</td><td>600</td></tr><tr><td>train/grad_norm</td><td>3.17144</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.6496</td></tr><tr><td>train_loss</td><td>2.04026</td></tr><tr><td>train_runtime</td><td>538.8614</td></tr><tr><td>train_samples_per_second</td><td>30.063</td></tr><tr><td>train_steps_per_second</td><td>3.897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-20250902-084523</strong> at: <a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning/runs/2s0700tj' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning/runs/2s0700tj</a><br> View project at: <a href='https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/llama-3-8B-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250902_084523-2s0700tj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286e9a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dec5401a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dd575",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325987b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import argparse\n",
    "import readline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7597df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the specified path.\n",
    "    \"\"\"\n",
    "    # Set dtype accordingly\n",
    "    torch_dtype = (\n",
    "        torch.bfloat16\n",
    "        if device == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "        else torch.float16\n",
    "    )\n",
    "\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)    \n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model_unsloth(model_name, max_seq_length=2048, load_in_4bit=False, dtype=None):\n",
    "    \"\"\" Load model and tokenizer with Unsloth optimizations \"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map=\"auto\"    \n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(\n",
    "    model, tokenizer, prompt, max_new_tokens=512, temperature=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Generate the response\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,        \n",
    "        # Stop generation when these tokens are encountered.        \n",
    "        #eos_token_id=[\n",
    "        #    tokenizer.eos_token_id,\n",
    "        #    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        #],\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=False, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return \"\"  # Streamer handles the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92a9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the inference script.\n",
    "    \"\"\"\n",
    "    \n",
    "    working_dir = './'\n",
    "    model_path = os.path.join(working_dir, \"unsloth_lab_outputs\")\n",
    "\n",
    "    prompt = \"I want you to act as a motivational coach. \"\n",
    "    max_new_tokens = 100\n",
    "    temperature = 0.2\n",
    "\n",
    "    # Determine the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    #model, tokenizer = load_model(model_path, device)\n",
    "    model, tokenizer = load_model_unsloth(model_path, \n",
    "                                          load_in_4bit=True)\n",
    "\n",
    "    # Single prompt mode\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    generate_response(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75c398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "==((====))==  Unsloth 2025.7.2: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "2025-09-02 11:37:02,981 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Prompt: I want you to act as a motivational coach. \n",
      "\n",
      "Response:\n",
      " I will provide some information about an individual, and your task is help them develop the mindset necessary for achieving their goals by providing positive affirmations or advice on how they can overcome any obstacles standing in way of success.\" My first suggestion request \"Hello! Today we are going discuss ways improve self-confidence through daily practices such as journaling, meditation etc.. What do think best practice start with?\" In reply write me only one sentence long answer that related my question. Do not explain anything else except what\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c36d92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
