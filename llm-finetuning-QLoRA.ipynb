{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82be6f",
   "metadata": {},
   "source": [
    "# <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "\n",
    "\n",
    "#### **What we'll build**:\n",
    "\n",
    "\n",
    "#### **Architecture**:\n",
    "\n",
    "\n",
    "#### **Constraints**:\n",
    "\n",
    " - None\n",
    "\n",
    "\n",
    "#### **Tools**:\n",
    "\n",
    " - Use local **ollama** model\n",
    "\n",
    "#### **Requirements**:\n",
    " - Make it work as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786816a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95db5",
   "metadata": {},
   "source": [
    "### Load the PEFT and Datasets libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99b5dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gpus availability\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e542ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d31d1",
   "metadata": {},
   "source": [
    "### Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b3109e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/workspace/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05c290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `test`\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#!huggingface-cli login --token $HF_TOKEN\n",
    "!hf auth login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9c685",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85558232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "#model_name = \"bigscience/bloom-7b1\"\n",
    "#target_modules = [\"query_key_value\"]\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b327ee5",
   "metadata": {},
   "source": [
    "To load the model, we need a configuration class that specifies how we want the quantization to be performed. Weâ€™ll achieve this with the BitesAndBytesConfig from the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82a9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ab01c",
   "metadata": {},
   "source": [
    "We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss.\n",
    "\n",
    "For the bnb_4bit_quant_type parameter, I've used the recommended value in the paper [QLoRA: Efficient Finetuning of Quantized LLMs.](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "Now, we can go ahead and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ddca0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4e866dc41d48bcbc21b27ee0a5c521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59a925443d94834b5d97b54e0339483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77e1d05cee146039abfbd650abfa736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6d29ce54564f13985d73a423b2eb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f82d8906c344478304c3336466cfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a8cd60c40b485ab3f8985bf8fbd8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a237127b6024496681472c97f0ecad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1128e1010c94c4d814b0b6811fc3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be129e53e7754686916241bc1d8140d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = {\"\":0}\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=device_map,\n",
    "                    use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fad4c",
   "metadata": {},
   "source": [
    "Now we have the quantized version of the model in memory. Yo can try to load the unquantized version to see if it's possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5833c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff67168d39534018bb6285933ca78b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d5513c6f90469993143d7665a408da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d22553f3e420eb53bd57b38a95a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7730211",
   "metadata": {},
   "source": [
    "#### Inference with the pre-trained model.\n",
    "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c50fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=False, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a312fd",
   "metadata": {},
   "source": [
    "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
    "\n",
    "I'm going to request the pre-trained model that acts like a motivational coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6434f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want you to act as a motivational coach. \\xa0You will need to be empathetic, supportive and understanding.\\nThe first part of the assignment is for us all (including myself)to write down three things that are important in our lives at this point\\nThis can include anything from family relationships or']\n"
     ]
    }
   ],
   "source": [
    "# Inference original model\n",
    "input_sentences = tokenizer(\"I want you to act as a motivational coach. \",\n",
    "                            return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "foundational_outputs_sentence = get_outputs(foundation_model,\n",
    "                                            input_sentences,\n",
    "                                            max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence,\n",
    "                             skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153506f",
   "metadata": {},
   "source": [
    "The answer is good enough, the models used is a really well trained Model. But we will try to improve the quality with a sort fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022e808",
   "metadata": {},
   "source": [
    "#### Preparing the Dataset.\n",
    "The Dataset useds is:\n",
    "\n",
    "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "554f1fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7441f73bd57a475d8da506ec0958abf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 203\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "#Create the Dataset to create prompts\n",
    "data = load_dataset(dataset)\n",
    "\n",
    "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "train_sample = data[\"train\"]#.select(range(50))\n",
    "\n",
    "del data\n",
    "train_sample = train_sample.remove_columns('act')\n",
    "\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a099f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': ['Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'], 'input_ids': [[128000, 52157, 499, 527, 459, 10534, 35046, 16131, 51920, 449, 6968, 264, 7941, 5226, 369, 264, 18428, 50596, 13, 578, 16945, 374, 311, 3665, 6743, 389, 279, 18428, 11, 3339, 1124, 34898, 320, 898, 8, 311, 5127, 11, 47005, 320, 2039, 8, 1193, 311, 279, 1732, 889, 27167, 279, 5226, 11, 323, 311, 1797, 1268, 1690, 3115, 279, 1984, 574, 6177, 13, 8000, 264, 22925, 488, 7941, 5226, 369, 420, 7580, 11, 2737, 279, 5995, 5865, 323, 38864, 369, 32145, 279, 5300, 9021, 13, 5321, 3493, 279, 2082, 323, 904, 9959, 41941, 311, 6106, 264, 2867, 8830, 315, 279, 8292, 13]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792eda06",
   "metadata": {},
   "source": [
    "#### Fine-Tuning.\n",
    "The first step will be to create a LoRA configuration object where we will set the variables that specify the characteristics of the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab11567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_MODULES\n",
    "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_config, PeftModel\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4, #As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training.\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
    "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0c096",
   "metadata": {},
   "source": [
    "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the value more parameters are trained, but it means that the model will be able to learn more complicated relations between inputs and outputs.\n",
    "\n",
    "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
    "\n",
    "**lora_alpha**. Ad bigger the number more weight have the LoRA activations, it means that the fine-tuning process will have more impac as bigger is this value.\n",
    "\n",
    "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
    "\n",
    "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
    "\n",
    "**task_type**. Indicates the task the model is beign trained for. In this case, text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94528d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to contain the Model\n",
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96cfb5",
   "metadata": {},
   "source": [
    "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d974f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TrainingArgs\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate=2e-4,  # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a858c8",
   "metadata": {},
   "source": [
    "Now we can train the model.\n",
    "To train the model we need:\n",
    "\n",
    "\n",
    "*   The Model.\n",
    "*   The training_args\n",
    "* The Dataset\n",
    "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
    "* The LoRA config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24aabe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac7cf60424c4580a9c61fbe08aca17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilverkonlambigue\u001b[0m (\u001b[33msilverkonlambigue-skd\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/llm-finetuning/wandb/run-20250901_091128-yss8qiw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silverkonlambigue-skd/huggingface/runs/yss8qiw9' target=\"_blank\">elated-disco-13</a></strong> to <a href='https://wandb.ai/silverkonlambigue-skd/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silverkonlambigue-skd/huggingface' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silverkonlambigue-skd/huggingface/runs/yss8qiw9' target=\"_blank\">https://wandb.ai/silverkonlambigue-skd/huggingface/runs/yss8qiw9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/130 01:05 < 2:18:38, 0.02 it/s, Epoch 0.08/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/145 06:46 < 57:51, 0.04 it/s, Epoch 0.55/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20/170 06:04 < 50:33, 0.05 it/s, Epoch 0.56/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36/205 08:43 < 43:21, 0.06 it/s, Epoch 0.85/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 52:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=255, training_loss=1.4651868633195466, metrics={'train_runtime': 3174.6122, 'train_samples_per_second': 0.32, 'train_steps_per_second': 0.08, 'total_flos': 9506274363113472.0, 'train_loss': 1.4651868633195466})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=foundation_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    peft_config=lora_config,\n",
    "    #dataset_text_field=\"prompt\",\n",
    "    #tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887dd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "peft_model_path = os.path.join(output_directory, f\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0dee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874ff670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6501"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In case you are having memory problems uncomment this lines to free some memory\n",
    "import gc\n",
    "import torch\n",
    "del foundation_model\n",
    "del trainer\n",
    "del train_sample\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e7e7a",
   "metadata": {},
   "source": [
    "#### Inference with the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8119fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import peft\n",
    "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
    "#import os\n",
    "\n",
    "device_map = {\"\":0}\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
    "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b52f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config2 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "817213c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e6be4460da4d97bc78e2bdbdc9a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the model\n",
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_model_path,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    is_trainable=False,\n",
    "    #load_in_4bit=True,\n",
    "    quantization_config=bnb_config2,\n",
    "    device_map='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141b304",
   "metadata": {},
   "source": [
    "#### Inference the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb53e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want you to act as a motivational coach.  I will provide some information about an individual looking for inspiration and guidance, such as their goals or challenges they are facing in life; your role is then help this person find the best ways of reaching those objectives through positive thinking techniques - offering advice on how']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = tokenizer(\"I want you to act as a motivational coach. \",\n",
    "                            return_tensors=\"pt\").to('cuda')\n",
    "foundational_outputs_sentence = get_outputs(loaded_model,\n",
    "                                            input_sentences,\n",
    "                                            max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence,\n",
    "                             skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbd3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
